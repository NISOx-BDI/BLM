{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FS_sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomMaullin/BLM/blob/master/FS_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIMtrhFKB3Ay",
        "colab_type": "text"
      },
      "source": [
        "# FS implementation in python\n",
        "\n",
        "This code implements the Fisher Scoring algorithm for estimating the parameters of linear mixed effects models as described in [Demidenko 2013](https://www.wiley.com/en-us/Mixed+Models%3A+Theory+and+Applications+with+R%2C+2nd+Edition-p-9781118091579)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFQ0-MpQuBm",
        "colab_type": "text"
      },
      "source": [
        "## Pip Installations\n",
        "\n",
        "Pip install everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX02P0KvBWJr",
        "colab_type": "code",
        "outputId": "3b4fbb5c-e216-472a-a80c-6ce9cc05e887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install matplotlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.5)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4JYRICVBtjl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Python Imports\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkTBWbRKQ5ah",
        "colab_type": "text"
      },
      "source": [
        "We need:\n",
        " - `numpy` for matrix handling.\n",
        " - `scipy` for sparse matrix functions.\n",
        " - `pandas` for quick reading and writing of csv files.\n",
        " - `os` and `sys` for basic commandline functions\n",
        " - `time` for timing functions.\n",
        " - `matplotlib` for making displays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tebSlxvBBruv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cvxopt\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import scipy.sparse\n",
        "import scipy.sparse.linalg\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS5zhoWCDZ8E",
        "colab_type": "text"
      },
      "source": [
        "## Toy Dataset\n",
        "\n",
        "This section read ins and formats a toy dataset. The files used here were generated in `R` and with **True** values (those with postfix `True`) being those used to generate the data and **Estimated** (those with postfix `REst`) values being the estimates `R`'s `lmer` package generated from this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy39zwuhkn4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a data directory\n",
        "if not os.path.isdir('/Data'):\n",
        "  os.mkdir('/Data')\n",
        "  \n",
        "os.chdir('/Data')\n",
        "\n",
        "# Clone small git repo containg some csv files.\n",
        "if not os.path.isdir('/Data/BLMM-testdata'):\n",
        "  !git clone https://github.com/TomMaullin/BLMM-testdata.git\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EWsCZjCQcc9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Z matrix\n",
        "\n",
        "The below reads in Z and makes an image of Z transpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoKOwHqcEKDT",
        "colab_type": "code",
        "outputId": "b84aff98-947e-400e-a547-bc24faeb1c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Read in random effects design matrix and convert it into it's sparse format in\n",
        "# cvxopt.\n",
        "Z_3col=pd.read_csv('/Data/BLMM-testdata/Z_3col.csv',header=None).values\n",
        "Z = scipy.sparse.csr_matrix((Z_3col[:,2].tolist(), \\\n",
        "                            ((Z_3col[:,0]-1).astype(np.int64), \\\n",
        "                             (Z_3col[:,1]-1).astype(np.int64))))\n",
        "\n",
        "# Create an image of Z'\n",
        "imshow(Z.toarray().transpose(), \\\n",
        "       interpolation='nearest', vmin=-5, vmax=5, aspect='auto')\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f57d4836470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAgIQR8AAkLQB4CAEPQBICD/D6dLkr+Yz5X4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkd2LdwKLWtP",
        "colab_type": "text"
      },
      "source": [
        "### Estimated Random Effects matrix\n",
        "\n",
        "The below reads in the Random effects variance predicted by `R`'s `lmer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOqQRAROqdkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in estimated variance\n",
        "RFXVar_REst = pd.read_csv('/Data/BLMM-testdata/estd_rfxvar.csv',header=None).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStOLwF_LlTE",
        "colab_type": "text"
      },
      "source": [
        "### Y vector\n",
        "\n",
        "The response vector is read in here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG8eWNdpPQOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=pd.read_csv('/Data/BLMM-testdata/Y.csv',header=None).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHm6DjdbPTvg",
        "colab_type": "text"
      },
      "source": [
        "### X matrix\n",
        "\n",
        "The fixed effects design matrix is read in here. It consists of an intercept and two random (Gaussian) columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqTKH4n_Po8e",
        "colab_type": "code",
        "outputId": "f43d58b2-6117-4589-bfe0-598ad70b8ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "X=pd.read_csv('/Data/BLMM-testdata/X.csv',header=None).values\n",
        "\n",
        "# Image of the first 20 rows of X\n",
        "imshow(X[1:20,:])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f57d0fc43c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFYAAAD8CAYAAADt0VN/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sXGD8qzskp",
        "colab_type": "text"
      },
      "source": [
        "### Number of Levels and Parameters\n",
        "\n",
        "The number of levels is given by a vector with one entry for each grouping factor. e.g. nlevels=[10,2] means there are 10 levels for factor 1 and 2 levels for factor 2. \n",
        "\n",
        "The number of parameters is given by a vector with one entry for each grouping factor. e.g. nparams=[3,4] means there are 3 variables for factor 1 and 4 variables for factor 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e5bUA2DztCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlevels = np.array([20,3])\n",
        "nparams = np.array([2,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0byKXygpCa1P",
        "colab_type": "text"
      },
      "source": [
        "### True b values\n",
        "\n",
        "The true recorded values of the random effects b vector in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwuuWSdCbCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_True=pd.read_csv('/Data/BLMM-testdata/true_b.csv',header=None).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvfns6c-CbPd",
        "colab_type": "text"
      },
      "source": [
        "### True beta values\n",
        "\n",
        "The true fixed effects parameters used to generate this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlvolWwvCbbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta_True=pd.read_csv('/Data/BLMM-testdata/true_beta.csv',header=None).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVyBWvQK2Gw5",
        "colab_type": "text"
      },
      "source": [
        "### Product Matrices\n",
        "\n",
        "All products of matrices are calculated beforehand as it is both more computationally efficient and also similar to the setting we are interested in. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e53HYCsj2G7I",
        "colab_type": "code",
        "outputId": "d5a6ef0c-b257-41c3-f50e-0966f700c57e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Z transpose Z\n",
        "print(Z.shape)\n",
        "ZtZ = np.matmul(Z.toarray().transpose(),Z.toarray()) # This works for products involving sparse\n",
        "# Sparse \n",
        "# ZtZ = Z.transpose() * Z\n",
        "\n",
        "# Z transpose X\n",
        "XtZ = np.matmul(X.transpose(),Z.toarray())\n",
        "\n",
        "# X transpose Z\n",
        "ZtX = np.matmul(Z.toarray().transpose(),X)\n",
        "\n",
        "# ZtY\n",
        "ZtY = np.matmul(Z.toarray().transpose(),Y)\n",
        "\n",
        "# YtZ\n",
        "YtZ = np.matmul(Y.transpose(),Z.toarray())\n",
        "\n",
        "# XtX\n",
        "XtX = np.matmul(X.transpose(),X)\n",
        "\n",
        "# XtY\n",
        "XtY = np.matmul(X.transpose(),Y)\n",
        "\n",
        "# YtX\n",
        "YtX = np.matmul(Y.transpose(),X)\n",
        "\n",
        "# YtX\n",
        "YtY = np.matmul(Y.transpose(),Y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAB3qmDMHa8",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "This section contains miscellaneous functions used to help the `FS` function including functions to work out the duplication matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVkJVMSo1fCF",
        "colab_type": "text"
      },
      "source": [
        "###Matrix to Vector function\n",
        "\n",
        "This function takes in a matrix and vectorizes it (i.e. transforms it to a vector of each of the columns of the matrix stacked on top of one another). Example:\n",
        "\n",
        "$$\\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\\\end{bmatrix} \\rightarrow \\begin{bmatrix} a \\\\ d \\\\ g \\\\ b \\\\ e \\\\ h \\\\ c \\\\ f \\\\ i \\end{bmatrix}$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBhQXRF01d9n",
        "colab_type": "code",
        "outputId": "32f0d120-fcff-4a21-d2f5-6dcbf1618d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "def mat2vec(matrix):\n",
        "  \n",
        "  #Return vectorised matrix\n",
        "  return(matrix.transpose().reshape(matrix.shape[0]*matrix.shape[1],1))\n",
        "\n",
        "# Example:\n",
        "matrix = np.random.randn(3,3)\n",
        "print(matrix)\n",
        "print(mat2vec(matrix))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.13554138 -1.86913126  0.13226088]\n",
            " [-0.77665361  1.28200347  1.29031668]\n",
            " [-1.23365759 -0.27247786  1.02978613]]\n",
            "[[-1.13554138]\n",
            " [-0.77665361]\n",
            " [-1.23365759]\n",
            " [-1.86913126]\n",
            " [ 1.28200347]\n",
            " [-0.27247786]\n",
            " [ 0.13226088]\n",
            " [ 1.29031668]\n",
            " [ 1.02978613]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcIJAyHi1eTj",
        "colab_type": "text"
      },
      "source": [
        "###Matrix to Vector function\n",
        "\n",
        "This function takes in a (symmetric, square) matrix and half-vectorizes it (i.e. transforms it to a vector of each of the columns of the matrix, below and including the diagonal, stacked on top of one another). Example:\n",
        "\n",
        "$$\\begin{bmatrix} a & b & c \\\\ b & d & e \\\\ c & e & f \\\\\\end{bmatrix} \\rightarrow \\begin{bmatrix} a \\\\ b \\\\ c \\\\ d \\\\ e \\\\ f \\end{bmatrix}$$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fpQzdwA3QGq",
        "colab_type": "code",
        "outputId": "6ad5c514-dbae-4762-e8e8-a514ebc57ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def mat2vech(matrix):\n",
        "  \n",
        "  # Get lower triangular indices\n",
        "  rowinds, colinds = np.tril_indices(matrix.shape[0]) #Try mat.transpose()[trilu]?\n",
        "  \n",
        "  # They're in the wrong order so we need to order them\n",
        "  # To do this we first hash them\n",
        "  indhash = colinds*matrix.shape[0]+rowinds\n",
        "  \n",
        "  # Sort permutation\n",
        "  perm=np.argsort(indhash)\n",
        "  \n",
        "  # Return vectorised half-matrix\n",
        "  return(np.array([matrix[rowinds[perm],colinds[perm]]]).transpose())\n",
        "\n",
        "# Example:\n",
        "matrix = np.random.randn(3,3)\n",
        "print(matrix*matrix.transpose())\n",
        "print(mat2vech(matrix*matrix.transpose()))\n",
        "\n",
        "#print(vech2mat(invDupMat(3) @ mat2vec(matrix*matrix.transpose())))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.01822102 0.00296636 0.89051255]\n",
            " [0.00296636 0.42588996 0.02318647]\n",
            " [0.89051255 0.02318647 0.13243242]]\n",
            "[[0.01822102]\n",
            " [0.00296636]\n",
            " [0.89051255]\n",
            " [0.42588996]\n",
            " [0.02318647]\n",
            " [0.13243242]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V79rfwHp9eNe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DmbjUOh9euy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vec2mat(vec):\n",
        "  \n",
        "  # Return matrix\n",
        "  return(vec.reshape(np.int64(np.sqrt(vec.shape[0])),np.int64(np.sqrt(vec.shape[0]))).transpose())\n",
        "\n",
        "# Example\n",
        "#vec = np.array([[1,2,3,4]]).transpose()\n",
        "#mat = vec2mat(vec)\n",
        "#print(vec)\n",
        "#print(mat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijBvOZsu9iKg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhitZciM9hcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vech2mat(vech):\n",
        "  \n",
        "  # dimension of matrix\n",
        "  n = np.int64((-1+np.sqrt(1+8*vech.shape[0]))/2)\n",
        "  matrix = np.zeros((n,n))\n",
        "  \n",
        "  # Get lower triangular indices\n",
        "  rowinds, colinds = np.tril_indices(matrix.shape[0])\n",
        "  \n",
        "  # They're in the wrong order so we need to order them\n",
        "  # To do this we first hash them\n",
        "  indhash = colinds*matrix.shape[0]+rowinds\n",
        "  \n",
        "  # Sort permutation\n",
        "  perm=np.argsort(indhash)\n",
        "  \n",
        "  # Assign values to lower half\n",
        "  matrix[rowinds[perm],colinds[perm]] = vech.reshape(vech.shape[0])\n",
        "  \n",
        "  # Assign values to upper half\n",
        "  matrix[colinds[perm],rowinds[perm]] = vech.reshape(vech.shape[0])\n",
        "  \n",
        "  # Return vectorised half-matrix\n",
        "  return(matrix)\n",
        "\n",
        "# Example:\n",
        "#vech = np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])\n",
        "#matrix = vech2mat(vech)\n",
        "#print(vech)\n",
        "#print(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd3_jaDe-MNy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qutTs-8S-MuT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vec2vech(vec):\n",
        "  \n",
        "  # Return vech\n",
        "  return(mat2vech(vec2mat(vec)))\n",
        "\n",
        "# Example\n",
        "#vec = np.array([[1],[2],[3],[2],[4],[5],[3],[5],[6]])\n",
        "#vech = vec2vech(vec)\n",
        "\n",
        "#print(vec)\n",
        "#print(vech)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Yp9RjT-PmL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8txTVtu-Pwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vech2vec(vech):\n",
        "  \n",
        "  # Return vec\n",
        "  return(mat2vec(vech2mat(vech)))\n",
        "\n",
        "# Example\n",
        "#vech = np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])\n",
        "#vec = vech2vec(vech)\n",
        "\n",
        "#print(vech)\n",
        "#print(vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG22u6mq9mQT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PD6f9bD9mgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dupMat(n):\n",
        "  \n",
        "  # Make vech of 1:(n(n+1)/2)\n",
        "  vech = np.arange(n*(n+1)/2)\n",
        "  \n",
        "  # Convert to vec\n",
        "  vec = vech2vec(vech)\n",
        "  \n",
        "  # Make D (sparse one hot encoded vec)\n",
        "  D = scipy.sparse.csr_matrix((np.ones(n**2),(np.arange(n**2),np.int64(vec).reshape(vec.shape[0]))))\n",
        "  \n",
        "  return(D)\n",
        "\n",
        "# Example\n",
        "#print(dupMat(3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVrM3CsGSIHL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FR-cwIkSIQJ",
        "colab_type": "code",
        "outputId": "a1760f1e-9806-4095-c43d-882716e5e049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "def invDupMat(n):\n",
        "  \n",
        "  \n",
        "  # Make vech of 1:(n(n+1)/2)\n",
        "  vech = np.arange(n*(n+1)/2)\n",
        "  \n",
        "  # Convert to vec\n",
        "  vec = np.int64(vech2vec(vech))\n",
        "  vec = vec.reshape(vec.shape[0])\n",
        "  \n",
        "  # Work out frequency of each entry\n",
        "  freq = 1/np.bincount(vec)\n",
        "  \n",
        "  # Work out duplication matrix\n",
        "  D = scipy.sparse.csr_matrix((freq[vec],(vec,np.arange(n**2))))\n",
        "  \n",
        "  return(D)\n",
        "\n",
        "# Example\n",
        "print(invDupMat(3))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 0)\t1.0\n",
            "  (1, 1)\t0.5\n",
            "  (1, 3)\t0.5\n",
            "  (2, 2)\t0.5\n",
            "  (2, 6)\t0.5\n",
            "  (3, 4)\t1.0\n",
            "  (4, 5)\t0.5\n",
            "  (4, 7)\t0.5\n",
            "  (5, 8)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2byzJx5Kf5fS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFCU4gxkf5r8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def blockInverse(matrix, blockSize, numBlocks):\n",
        "\n",
        "  invMatrix = scipy.sparse.csr_matrix((np.array([]), (np.array([]),np.array([]))),shape=matrix.shape)\n",
        "  \n",
        "  # For each level, invert the corresponding block on the diagonal\n",
        "  for i in range(numBlocks):\n",
        "    \n",
        "    # The block is nparams by nparams\n",
        "    blockInds = np.ix_(np.arange(i*blockSize,(i+1)*blockSize),np.arange(i*blockSize,(i+1)*blockSize))\n",
        "    \n",
        "    # Get the block\n",
        "    block = matrix[blockInds]\n",
        "    \n",
        "    # Replace it with it's inverse\n",
        "    invMatrix[blockInds]=scipy.sparse.linalg.inv(block)\n",
        "    \n",
        "  return(invMatrix)\n",
        "\n",
        "# Example - need to have loaded in data first\n",
        "\n",
        "# Get ZtZ just for the first grouping factor\n",
        "firstFactorIndices = np.ix_(np.arange(nlevels[0]*nparams[0]),np.arange(nlevels[0]*nparams[0]))\n",
        "ZtZ_f1 = ZtZ[firstFactorIndices]\n",
        "\n",
        "# Compute the block inverse for ZtZ_f1\n",
        "#t1 = time.time()\n",
        "#ZtZ_f1_inv = blockInverse(matrix=ZtZ_f1, blockSize=nparams[0], numBlocks=nlevels[0])\n",
        "#t2 = time.time()\n",
        "#blockInverse_time = t2-t1\n",
        "\n",
        "# Compare it to the inverse scipy would calculate\n",
        "#t1 = time.time()\n",
        "#ZtZ_f1_inv_sp = scipy.sparse.linalg.inv(ZtZ_f1)\n",
        "#t2 = time.time()\n",
        "#scipyInverse_time = t2-t1\n",
        "\n",
        "#print(blockInverse_time)\n",
        "#print(scipyInverse_time)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BdQ3jh7x71k",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77gBbWIAx8AZ",
        "colab_type": "code",
        "outputId": "21455e2d-122d-4d05-d8aa-e0ff121f91b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "def recursiveInverse(M, nparams, nlevels):\n",
        "  \n",
        "  # Work out qc\n",
        "  qc = nparams[-1]*nlevels[-1]\n",
        "  \n",
        "  # Make q\n",
        "  q = M.shape[0]\n",
        "  \n",
        "  # Get A, B and C where M=[[A,B],[B',C]]\n",
        "  # A\n",
        "  A_inds = np.ix_(np.arange(0,(q-qc)),np.arange(0,(q-qc)))\n",
        "  A = M[A_inds]\n",
        "  \n",
        "  # B\n",
        "  B_inds = np.ix_(np.arange(0,(q-qc)),np.arange((q-qc),q))\n",
        "  B = M[B_inds].toarray() # B is dense\n",
        "  \n",
        "  # C\n",
        "  C_inds = np.ix_(np.arange((q-qc),q),np.arange((q-qc),q))\n",
        "  C = M[C_inds].toarray() # C is small and now only involved in dense mutliplys\n",
        "  \n",
        "  # Recursive inverse A\n",
        "  if nparams[:-1].shape[0] > 1:\n",
        "    \n",
        "    Ainv = recursiveInverse(A, nparams[:-1], nlevels[:-1]).toarray()\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    #Ainv = blockInverse(A, nparams[0], nlevels[0]) - much slower\n",
        "    Ainv = scipy.sparse.linalg.inv(scipy.sparse.csc_matrix(A)).toarray()\n",
        "  \n",
        "  # Schur complement\n",
        "  S = C-np.matmul(np.matmul(B.transpose(),Ainv),B)\n",
        "  Sinv = np.linalg.inv(S)\n",
        "  \n",
        "  # Top Left Hand Side of inverse\n",
        "  TLHS = Ainv + np.matmul(np.matmul(np.matmul(np.matmul(Ainv,B),Sinv),B.transpose()),Ainv)\n",
        "  \n",
        "  \n",
        "  # Top Right Hand Side of inverse\n",
        "  TRHS = -np.matmul(np.matmul(Ainv,B),Sinv)\n",
        "  \n",
        "  \n",
        "  # Bottom Right Hand Side of inverse\n",
        "  BRHS = Sinv\n",
        "  \n",
        "  # Join together\n",
        "  top = np.hstack((TLHS,TRHS))\n",
        "  bottom = np.hstack((TRHS.transpose(), BRHS))\n",
        "  \n",
        "  # Make Minv\n",
        "  Minv = np.vstack((top, bottom))\n",
        "  \n",
        "  return(Minv)\n",
        "\n",
        "# Example\n",
        "t1 = time.time()\n",
        "ZtZinv_rec = recursiveInverse(ZtZ, nparams, nlevels)\n",
        "t2 = time.time()\n",
        "inv_rec_time = t2-t1\n",
        "\n",
        "t1 = time.time()\n",
        "ZtZinv_sp = scipy.sparse.linalg.inv(scipy.sparse.csc_matrix(ZtZ))\n",
        "t2 = time.time()\n",
        "inv_sp_time = t2-t1\n",
        "\n",
        "t1 = time.time()\n",
        "ZtZinv_np = np.linalg.inv(ZtZ.toarray())\n",
        "t2 = time.time()\n",
        "inv_np_time = t2-t1\n",
        "\n",
        "\n",
        "print('Distance (norm) from identity (scipy)')\n",
        "print(np.linalg.norm(np.matmul(ZtZinv_sp.toarray(),ZtZ.toarray())-np.eye(ZtZ.shape[0])))\n",
        "print('Distance (norm) from identity (numpy)')\n",
        "print(np.linalg.norm(np.matmul(ZtZinv_np,ZtZ.toarray())-np.eye(ZtZ.shape[0])))\n",
        "print('Distance (norm) from identity (rec)')\n",
        "print(np.linalg.norm(np.matmul(ZtZinv_rec,ZtZ.toarray())-np.eye(ZtZ.shape[0])))\n",
        "\n",
        "print(inv_sp_time)\n",
        "print(inv_np_time)\n",
        "print(inv_rec_time)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6dc87c55f0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mZtZinv_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecursiveInverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZtZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlevels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0minv_rec_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-6dc87c55f0a9>\u001b[0m in \u001b[0;36mrecursiveInverse\u001b[0;34m(M, nparams, nlevels)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mB_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mqc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mqc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mB_inds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B is dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPgPFpnOLxdG",
        "colab_type": "text"
      },
      "source": [
        "#### Sum of square residuals\n",
        "\n",
        "The function below calculates the sum of the square residuals, $e^Te$, using the below formula:\n",
        "\n",
        "$$e^Te = (Y-X\\beta)^T(Y-X\\beta)$$ \n",
        "$$=Y^TY - 2Y^TX\\beta + \\beta^T X^TX \\beta$$\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `YtX`: $Y$ transpose multiplied by $X$ ($Y^TX$ in the above notation).\n",
        " - `YtY`: $Y$ transpose multiplied by $Y$ ($Y^TY$ in the above notation).\n",
        " - `XtX`: $X$ transpose multiplied by $X$ ($X^TX$ in the above notation).\n",
        " - `beta`: An estimate of the parameter vector ($\\beta$ in the above notation).\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "\n",
        " - `ete`: The sum of square residuals ($e^Te$ in the above notation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK0i7TkgNNZD",
        "colab_type": "code",
        "outputId": "6814cbab-570a-488a-8ce9-1b3c539bd9b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def ssr(YtX, YtY, XtX, beta):\n",
        "  \n",
        "  # Return the sum of squared residuals\n",
        "  return(YtY - 2*YtX @ beta + beta.transpose() @ XtX @ beta)\n",
        "\n",
        "t1 = time.time()\n",
        "ssr(YtX, YtY, XtX, np.array([[1],[2],[3]]))\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0010385513305664062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO_68uYYxCvf",
        "colab_type": "text"
      },
      "source": [
        "#### Get Factor/Level Indices\n",
        "\n",
        "This function gives the indices of the columns of the $Z$ matrix which correspond to factor $k$ level $j$. \n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `k`: The grouping factor we need the columns of.\n",
        " - `j`: The level of the grouping factor $k$ which we are interested in.\n",
        " - `nlevels`: A vector containing the number of levels for each factor, e.g. `nlevels=[3,4]` would mean the first factor has 3 levels and the second factor has 4 levels.\n",
        " - `nparams`: A vector containing the number of parameters for each factor, e.g. `nlevels=[2,1]` would mean the first factor has 2 parameters and the second factor has 1 parameter.\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `Ikj`: The indices of the columns of $Z$ corresponding to factor $k$ level $j$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSrodNh0zI0_",
        "colab_type": "code",
        "outputId": "bba75176-43c2-466c-bdf9-d3898a17252e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numba\n",
        "\n",
        "@numba.jit\n",
        "# k and j are both zero indexed\n",
        "def faclev_indices(k, j, nlevels, nparams):\n",
        "  \n",
        "  # Work out the starting point of the indices\n",
        "  start = np.concatenate((np.array([0]), np.cumsum(nlevels*nparams)))[k] + nparams[k]*j\n",
        "  \n",
        "  # work out the end point of the indices\n",
        "  end = start + nparams[k]\n",
        "  \n",
        "  return(np.arange(start, end))\n",
        "\n",
        "t1 = time.time()\n",
        "faclev_indices(0, 1, nlevels, nparams)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5320243835449219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYv59en2gpt3",
        "colab_type": "code",
        "outputId": "bb089d90-ffbe-4b31-ff9b-6c738ab57ab1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = time.time()\n",
        "faclev_indices(0, 1, nlevels, nparams)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.225440979003906e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xeRsXZBJUGd",
        "colab_type": "text"
      },
      "source": [
        "#### Initial Sigma\n",
        "\n",
        "The function below returns an initial estimate for the Fixed Effects Variance, $\\sigma^2$. The estimator used is based on the suggested OLS estimator in Demidenko (2012) and is given by:\n",
        "\n",
        "$$\\hat{\\sigma}^2_{OLS}=\\frac{1}{n}(Y-X\\beta)^T(Y-X\\beta)$$\n",
        "$$=\\frac{1}{n}e^Te$$\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `ete`: The sum of square residuals ($e^Te$ in the above notation).\n",
        " - `n`: The total number of observations ($n$ in the above notation).\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `sigma2`: The OLS estimate of $\\sigma^2$ ($\\hat{\\sigma}^2_{OLS}$ in the above notation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_STdZ1mNLWbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initSigma2(ete, n):\n",
        "\n",
        "  # Return the OLS estimate of sigma\n",
        "  return(1/n*ete[0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdtIeA0NNsZl",
        "colab_type": "text"
      },
      "source": [
        "#### Initial D_k\n",
        "\n",
        "The function below returns an initial estimate for the Random Effects Variance matrix for the $k^{th}$ grouping factor, $D_k$. The estimator used is an adaption of the suggested estimator in Demidenko (2012) and is given by:\n",
        "\n",
        "$$vec(\\hat{D}_{k})=\\bigg[\\sum_{j=1}^{l_k}(Z_{(k,j)}^TZ_{(k,j)}) \\otimes (Z_{(k,j)}^TZ_{(k,j)})\\bigg]^{-1}vec\\bigg(\\sum_{j=1}^{l_k}[\\hat{\\sigma}^{-2}_{OLS}Z_{(k,j)}^Tee^TZ_{(k,j)} - Z_{(k,j)}^TZ_{(k,j)}]\\bigg)$$\n",
        "\n",
        "Or:\n",
        "\n",
        "$$\\hat{D}_{k}=matrix\\bigg(\\bigg[\\sum_{j=1}^{l_k}(Z_{(k,j)}^TZ_{(k,j)}) \\otimes (Z_{(k,j)}^TZ_{(k,j)})\\bigg]^{-1}vec\\bigg(\\sum_{j=1}^{l_k}[\\hat{\\sigma}^{-2}_{OLS}Z_{(k,j)}^Tee^TZ_{(k,j)} - Z_{(k,j)}^TZ_{(k,j)}]\\bigg)\\bigg)$$\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `k`: The grouping factor we wish to estimate $D$ for ($k$ in the above notation)\n",
        " - `lk`: The number of levels belonging to grouping factor $k$ ($l_k$ in the above notation).\n",
        " - `ZtZ`: The $Z$ matrix transposed and then multiplied by itself ($Z^TZ$ in the above notation).\n",
        " - `Zte`: The $Z$ matrix transposed and then multiplied by the OLS residuals ($Z^Te=Z^T(Y-X\\beta)$ in the above notation).\n",
        " - `sigma2`: The OLS estimate of $\\sigma^2$ ($\\hat{\\sigma}^2_{OLS}$ in the above notation).\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `Dkest`: The inital estimate of $D_k$ ($\\hat{D}_k$ in the above notation).\n",
        "\n",
        "\n",
        "###CHECK DERIVATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0J2CFTrNrhq",
        "colab_type": "code",
        "outputId": "1c072a30-2d77-404c-abde-47644db083f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numba\n",
        "\n",
        "@numba.jit\n",
        "def initDk(k, lk, ZtZ, Zte, sigma2):\n",
        "  \n",
        "  # Initalize D to zeros\n",
        "  invSig2ZteetZminusZtZ = np.zeros((nparams[k],nparams[k]))\n",
        "  \n",
        "  # For each level j we need to add a term\n",
        "  for j in np.arange(nlevels[k]):\n",
        "    \n",
        "    Ikj = faclev_indices(k, j, nlevels, nparams)\n",
        "\n",
        "    # Work out Z_(k, j)'Z_(k, j)\n",
        "    ZkjtZkj = ZtZ[np.ix_(Ikj,Ikj)]\n",
        "    \n",
        "    # Work out Z_(k,j)'e\n",
        "    Zkjte = Zte[Ikj,:]\n",
        "    \n",
        "    if j==0:\n",
        "      \n",
        "      # Add first Z_(k,j)'Z_(k,j) kron Z_(k,j)'Z_(k,j)\n",
        "      ZtZkronZtZ = np.kron(ZkjtZkj,ZkjtZkj.transpose())\n",
        "      \n",
        "      # Add first \\sigma^{-2}Z'ee'Z - Z_(k,j)'Z_(k,j)\n",
        "      invSig2ZteetZminusZtZ = 1/sigma2*(Zkjte @ Zkjte.transpose()) - ZkjtZkj\n",
        "      \n",
        "    else:\n",
        "      \n",
        "      # Add next Z_(k,j)'Z_(k,j) kron Z_(k,j)'Z_(k,j)\n",
        "      ZtZkronZtZ = ZtZkronZtZ + np.kron(ZkjtZkj,ZkjtZkj.transpose())\n",
        "      \n",
        "      # Add next \\sigma^{-2}Z'ee'Z - Z_(k,j)'Z_(k,j)\n",
        "      invSig2ZteetZminusZtZ = invSig2ZteetZminusZtZ + 1/sigma2*(Zkjte @ Zkjte.transpose()) - ZkjtZkj\n",
        "  \n",
        "  # Work out the final term.\n",
        "  Dkest = vec2mat(np.linalg.inv(ZtZkronZtZ) @ mat2vec(invSig2ZteetZminusZtZ)) \n",
        "  \n",
        "  return(Dkest)\n",
        "\n",
        "Zte = ZtY-ZtX @ np.array([[1],[2],[3]])\n",
        "\n",
        "t1 = time.time()\n",
        "initDk(1, nlevels[1], ZtZ, Zte , 1)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "\n",
        "t1 = time.time()\n",
        "initDk(1, nlevels[1], ZtZ, Zte , 1)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8649301528930664\n",
            "0.0005714893341064453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjypr01QTJh5",
        "colab_type": "text"
      },
      "source": [
        "#### Non-negative Definite D\n",
        "\n",
        "The below function takes in a covariance matrix $D$ and finds nearest projection onto the space of non-negative definite matrices $\\mathbb{D}_+$. It uses the following method taken from Demidenko (2012), page 105:\n",
        "\n",
        "If $D$ is non-negative definite and has eigenvalue decomposition $D=P\\Lambda P^T$ it's closest projection into $\\mathbb{D}_+$ is defined by the matrix below:\n",
        "\n",
        "$$\\hat{D}_+ = P\\Lambda_+P'$$\n",
        "\n",
        "Where $\\Lambda_+$ is defined by the elementwise maximum of $\\Lambda$ and 0; i.e. $\\Lambda_{+(i,j)} = max(\\Lambda_{+(i,j)},0)$.\n",
        "\n",
        "Note: This is not to be confused with the generalized inverse of the duplication matrix $\\mathcal{D}^+$.\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `D`: A square symmetric matrix.\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `D_nnd`: The nearest projection of $D$ onto the space of non-negative definite matrices $\\mathbb{D}_+$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2rW_uOBTJ0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def makeDnnd(D):\n",
        "  \n",
        "  # Check if we have negative eigenvalues\n",
        "  if not np.all(np.linalg.eigvals(D)>0):\n",
        "  \n",
        "    # If we have negative eigenvalues\n",
        "    eigvals,eigvecs = np.linalg.eigh(D)\n",
        "    \n",
        "    # Work out elementwise max of lambda and 0\n",
        "    lamplus = np.diag(np.maximum(eigvals,0))\n",
        "    \n",
        "    # Work out D+\n",
        "    D_nnd = eigvecs @ lamplus @ np.linalg.inv(eigvecs)\n",
        "    \n",
        "  else:\n",
        "    \n",
        "    # D is already non-negative in this case\n",
        "    D_nnd = D\n",
        "    \n",
        "  return(D_nnd)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70shU6hKlDMR",
        "colab_type": "text"
      },
      "source": [
        "#### Derivative of $l$ with respect to $\\beta$\n",
        "\n",
        "The below function calculates the derivative of the log likelihood with respect to $\\beta$. This is given by the following equation:\n",
        "\n",
        "$$\\frac{\\delta l}{\\delta \\beta} = \\sigma^{-2}X'(I+ZDZ')^{-1}(Y-X\\beta)$$\n",
        "$$ = \\sigma^{-2}X'(I-ZD(I+Z'ZD)^{-1}Z')(Y-X\\beta)$$\n",
        "$$ = \\sigma^{-2}X'(Y-X\\beta)-X'ZD(I+Z'ZD)^{-1}Z'(Y-X\\beta) $$\n",
        "$$ = \\sigma^{-2}X'e-X'ZD(I+Z'ZD)^{-1}Z'e$$\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `XtZ`: The $X$ matrix transposed and then multiplied by Z ($X^TZ$ in the above notation).\n",
        " - `Zte`: The $Z$ matrix transposed and then multiplied by the OLS residuals ($Z^Te=Z^T(Y-X\\beta)$ in the above notation).\n",
        " - `sigma2`: The fixed effects variance ($\\sigma^2$ in the above notation).\n",
        " - `DinvIplusZtZD`: The product $D(I+Z'ZD)^{-1}$.\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `dldb`: The derivative of $l$ with respect to $\\beta$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX6LfLOBlDVn",
        "colab_type": "code",
        "outputId": "d618bb87-3306-4cfd-dcb7-3571197c3fb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "def get_dldB(sigma2, Xte, XtZ, DinvIplusZtZD, Zte):\n",
        "  \n",
        "  # Return the derivative\n",
        "  return(1/sigma2*(Xte - (XtZ @ DinvIplusZtZD @ Zte)))\n",
        "\n",
        "### Test example\n",
        "\n",
        "p = X.shape[1]\n",
        "beta = np.random.randn(p,1)\n",
        "\n",
        "sigma2=1.1\n",
        "D = np.array([])\n",
        "q=Z.shape[1]\n",
        "n=Z.shape[0]\n",
        "for i in np.arange(len(nparams)):\n",
        "  \n",
        "  for j in np.arange(nlevels[i]):\n",
        "    \n",
        "    if i == 0 and j == 0:\n",
        "\n",
        "      D = initDk(i, nlevels[i], ZtZ, Zte, sigma2)\n",
        "\n",
        "    else:\n",
        "\n",
        "      D = scipy.linalg.block_diag(D, initDk(i, nlevels[i], ZtZ, Zte, sigma2))\n",
        "      \n",
        "IplusZtZD = np.eye(q) + ZtZ @ D\n",
        "DinvIplusZtZD = D @ np.linalg.inv(IplusZtZD)\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "dldb1 = (sigma2)**(-1)*(X.transpose() @ np.linalg.inv(np.eye(n) + Z @ D @ Z.transpose()) @ (Y - X @ beta))\n",
        "\n",
        "Xte = XtY- XtX @ beta\n",
        "\n",
        "Zte = ZtY- ZtX @ beta\n",
        "\n",
        "DinvIplusZtZD = D @ np.linalg.inv(np.eye(q) + ZtZ @ D)\n",
        "\n",
        "dldb2 = get_dldB(sigma2, Xte, XtZ, DinvIplusZtZD, Zte)\n",
        "\n",
        "print(dldb1)\n",
        "print(dldb2)\n",
        "\n",
        "\n",
        "print(dldb2-dldb1)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 3)\n",
            "[[1.93555086e-01]\n",
            " [1.15522847e+03]\n",
            " [2.28534201e+03]]\n",
            "[[1.93555086e-01]\n",
            " [1.15522847e+03]\n",
            " [2.28534201e+03]]\n",
            "[[-2.39530590e-10]\n",
            " [-3.92567472e-08]\n",
            " [ 5.93772711e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pL8p641GAgxF"
      },
      "source": [
        "#### Derivative of $l$ with respect to $\\sigma^2$\n",
        "\n",
        "The below function calculates the derivative of the log likelihood with respect to $\\beta$. This is given by the following equation:\n",
        "\n",
        "$$\\frac{\\delta l}{\\delta \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}(Y-X\\beta)'(I+ZDZ')^{-1}(Y-X\\beta)$$\n",
        "$$  = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}e'(I+ZDZ')^{-1}e$$\n",
        "$$  = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}e'(I-ZD(I+ZZ'D)^{-1}Z')e$$\n",
        "$$  = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4}(e'e-e'ZD(I+ZZ'D)^{-1}Z'e)$$\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `n`: The number of observations.\n",
        " - `ete`: The OLS residuals transposed and then multiplied by themselvess ($e^Te=(Y-X\\beta)^T(Y-X\\beta)$ in the above notation).\n",
        " - `Zte`: The $Z$ matrix transposed and then multiplied by the OLS residuals ($Z^Te=Z^T(Y-X\\beta)$ in the above notation).\n",
        " - `sigma2`: The fixed effects variance ($\\sigma^2$ in the above notation).\n",
        " - `DinvIplusZtZD`: The product $D(I+Z'ZD)^{-1}$.\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `dldsigma2`: The derivative of $l$ with respect to $\\sigma^2$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3102583a-281b-48aa-86ee-86ff87987ae1",
        "id": "R0T-_zfJAhRK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "def get_dldsigma2(n, ete, Zte, sigma2, DinvIplusZtZD):\n",
        "  \n",
        "  return(-n/(2*sigma2) + 1/(2*(sigma2**2))*(ete - (Zte.transpose() @ DinvIplusZtZD @ Zte)))\n",
        "\n",
        "\n",
        "Zte = Z.toarray().transpose() @ (Y - X @ beta)\n",
        "ete = (Y - X @ beta).transpose() @ (Y - X @ beta)\n",
        "DinvIplusZtZD = D @ np.linalg.inv(np.eye(q) + ZtZ @ D)\n",
        "\n",
        "t1 = time.time()\n",
        "dldsigma2_1 = get_dldsigma2(n, ete, Zte, sigma2, DinvIplusZtZD)\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "dldsigma2_2 = -n/(2*sigma2) + 1/(2*(sigma2**2))*(Y - X @ beta).transpose() @ np.linalg.inv(np.eye(n) + (Z.toarray() @ D @ Z.toarray().transpose())) @ (Y - X @ beta)\n",
        "\n",
        "print(dldsigma2_1)\n",
        "print(dldsigma2_2)\n",
        "print(dldsigma2_1-dldsigma2_2)\n",
        "\n",
        "print(np.linalg.inv(np.eye(n) + (Z @ D @ Z.transpose())))\n",
        "print(np.eye(n) - Z.toarray() @ D @ np.linalg.inv(np.eye(q) + ZtZ @ D) @ Z.toarray().transpose())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00012540817260742188\n",
            "[[-2.33521159]]\n",
            "[[10.97946868]]\n",
            "[[-13.31468027]]\n",
            "[[ 9.05569208e-01 -4.46146506e-02  1.92709693e-02 ...  3.49241881e-05\n",
            "   3.57009943e-05  3.96111077e-04]\n",
            " [-4.42812428e-02  9.66737543e-01 -6.33330291e-03 ... -6.26483911e-03\n",
            "   2.22355156e-03  5.71088528e-04]\n",
            " [ 1.88888446e-02 -6.27517014e-03  9.56050899e-01 ...  1.07749877e-03\n",
            "   8.43216493e-04  1.16499462e-03]\n",
            " ...\n",
            " [ 4.01576600e-05 -6.27041686e-03  1.08893874e-03 ...  9.67295703e-01\n",
            "  -3.67083742e-02 -1.51959799e-02]\n",
            " [ 3.62751901e-05  2.23650686e-03  8.37377586e-04 ... -3.68445061e-02\n",
            "   8.96421636e-01 -4.68844100e-03]\n",
            " [ 3.93917453e-04  5.51442963e-04  1.15986812e-03 ... -1.52108335e-02\n",
            "  -5.08784420e-03  9.73739640e-01]]\n",
            "[[ 9.05978359e-01 -4.44241621e-02  1.92446573e-02 ...  3.83422612e-05\n",
            "   3.95035872e-05  3.91288846e-04]\n",
            " [-4.44248288e-02  9.66705375e-01 -6.23251256e-03 ... -6.26339238e-03\n",
            "   2.23376666e-03  5.58347037e-04]\n",
            " [ 1.92445376e-02 -6.23262853e-03  9.55891271e-01 ...  1.08577047e-03\n",
            "   8.39300076e-04  1.16230133e-03]\n",
            " ...\n",
            " [ 3.80514332e-05 -6.26353813e-03  1.08593848e-03 ...  9.67261314e-01\n",
            "  -3.67735205e-02 -1.52258807e-02]\n",
            " [ 3.98281014e-05  2.23363952e-03  8.39946455e-04 ... -3.67750693e-02\n",
            "   8.96636256e-01 -4.95415367e-03]\n",
            " [ 3.91712422e-04  5.58318962e-04  1.16227422e-03 ... -1.52255500e-02\n",
            "  -4.95297937e-03  9.73806971e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0yUC5yE1qin",
        "colab_type": "text"
      },
      "source": [
        "#### Derivative of $l$ with respect to $D_k$\n",
        "\n",
        "The below function calculates the derivative of the log likelihood with respect to $D_k$, the random effects covariance matrix for factor $k$. This is given by the following equation:\n",
        "\n",
        "$$\\frac{\\delta l}{\\delta D_k} = \\frac{1}{2}\\sum_{j=1}^{l_k}(T_{(k,j)}u)(T_{(k,j)}u)'-\\frac{1}{2}\\sum_{j=1}^{l_k}T_{(k,j)}T_{(k,j)}'$$\n",
        "\n",
        "Where $T_{(i,j)}=Z'_{(i,j)}(I+ZDZ')^{-\\frac{1}{2}}$ and $u=\\sigma^{-1}(I+ZDZ')^{-\\frac{1}{2}}(Y-X\\beta)\\sim N(0,\\mathbb{I})$.\n",
        "\n",
        "$$= \\frac{1}{2\\sigma^2}\\sum_{j=1}^{l_k}Z'_{(k,j)}(I+ZDZ')^{-1}ee'(I+ZDZ')^{-1}Z_{(k,j)} - \\frac{1}{2}\\sum_{j=1}^{l_k}Z'_{(k,j)}(I+ZDZ')^{-1}Z_{(k,j)}$$\n",
        "\n",
        "$$= \\frac{1}{2\\sigma^2}\\sum_{j=1}^{l_k}Z'_{(k,j)}(I-ZD(I+Z'ZD)^{-1}Z')ee'(I-ZD(I+Z'ZD)^{-1}Z')'Z_{(k,j)} - \\frac{1}{2}\\sum_{j=1}^{l_k}Z'_{(k,j)}(I-ZD(I+Z'ZD)^{-1})Z_{(k,j)}$$\n",
        "\n",
        "$$= \\frac{1}{2\\sigma^2}\\sum_{j=1}^{l_k}(Z'_{(k,j)}e-Z'_{(k,j)}ZD(I+Z'ZD)^{-1}Z'e)(Z'_{(k,j)}e-Z'_{(k,j)}ZD(I+Z'ZD)^{-1}Z'e)' - \\frac{1}{2}\\sum_{j=1}^{l_k}Z'_{(k,j)}Z_{(k,j)}-Z'_{(k,j)}ZD(I+Z'ZD)^{-1}Z'Z_{(k,j)}$$\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `k`: The factor we wish to estimate the derivative of the covariance matrix of.\n",
        " - `nlevels`: A vector containing the number of levels for each factor, e.g. `nlevels=[3,4]` would mean the first factor has 3 levels and the second factor has 4 levels.\n",
        " - `nparams`: A vector containing the number of parameters for each factor, e.g. `nlevels=[2,1]` would mean the first factor has 2 parameters and the second factor has 1 parameter.\n",
        " - `ZtZ`: The $Z$ matrix transposed and then multiplied by itself ($Z^TZ$ in the above notation).\n",
        " - `Zte`: The $Z$ matrix transposed and then multiplied by the OLS residuals ($Z^Te=Z^T(Y-X\\beta)$ in the above notation).\n",
        " - `sigma2`: The fixed effects variance ($\\sigma^2$ in the above notation).\n",
        " - `DinvIplusZtZD`: The product $D(I+Z'ZD)^{-1}$.\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `dldD`: The derivative of $l$ with respect to $D_k$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D43Adi61quy",
        "colab_type": "code",
        "outputId": "ac49dd70-c441-4e43-c139-058b12311e3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import numba\n",
        "\n",
        "@numba.jit\n",
        "def get_dlDk(k, nlevels, nparams, ZtZ, Zte, sigma2, DinvIplusZtZD):\n",
        "\n",
        "  # Initalize the derivative to zeros\n",
        "  dldDk = np.zeros((nparams[k],nparams[k]))\n",
        "\n",
        "  # For each level j we need to add a term\n",
        "  for j in np.arange(nlevels[k]):\n",
        "\n",
        "    # Get the indices for the kth factor jth level\n",
        "    Ikj = faclev_indices(k, j, nlevels, nparams)\n",
        "\n",
        "    # Get (the kj^th columns of Z)^T multiplied by Z\n",
        "    Z_kjtZ = ZtZ[Ikj,:]\n",
        "    Z_kjte = Zte[Ikj,:]\n",
        "\n",
        "    # Get the first term of the derivative\n",
        "    invZ_kjtVinve = Z_kjte - (Z_kjtZ @ DinvIplusZtZD @ Zte)\n",
        "    firstterm = 1/sigma2 * (invZ_kjtVinve @ invZ_kjtVinve.transpose())\n",
        "    \n",
        "    # Get (the kj^th columns of Z)^T multiplied by (the kj^th columns of Z)\n",
        "    Z_kjtZ_kj = ZtZ[np.ix_(Ikj,Ikj)]\n",
        "    secondterm = Z_kjtZ_kj - (Z_kjtZ @ DinvIplusZtZD @ Z_kjtZ.transpose())\n",
        "      \n",
        "    if j == 0:\n",
        "      \n",
        "      # Start a running sum over j\n",
        "      dldDk = firstterm - secondterm\n",
        "      \n",
        "    else:\n",
        "    \n",
        "      # Add these to the running sum\n",
        "      dldDk = dldDk + firstterm - secondterm\n",
        "\n",
        "  # Halve the sum (the coefficient of a half was not included in the above)\n",
        "  dldDk = dldDk/2\n",
        "\n",
        "  # Store it in the dictionary\n",
        "  return(dldDk)\n",
        "\n",
        "IplusZtZD = np.eye(q) + ZtZ @ D\n",
        "DinvIplusZtZD = D @ np.linalg.inv(IplusZtZD)\n",
        "\n",
        "ZtZ = Z.toarray().transpose() @ Z.toarray()\n",
        "Zte = Z.toarray().transpose() @ (Y - X @ beta)\n",
        "\n",
        "k=0\n",
        "t1 = time.time()\n",
        "dldDk = get_dlDk(k, nlevels, nparams, ZtZ, Zte, sigma2, DinvIplusZtZD)\n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "t1 = time.time()\n",
        "dldDk = get_dlDk(k, nlevels, nparams, ZtZ, Zte, sigma2, DinvIplusZtZD)\n",
        "t2 = time.time()\n",
        "print(t2-t1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6593673229217529\n",
            "0.0009176731109619141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW-p5bOy4K-u",
        "colab_type": "text"
      },
      "source": [
        "#### Covariance of $\\frac{\\delta l}{\\delta \\text{vech}(D_{k_1})}$ and $\\frac{\\delta l}{\\delta \\text{vech}(D_{k_2})}$\n",
        "\n",
        "The below function calculates the covariance between the derivative of the log likelihood with respect to $\\text{vech}(D_{k_1})$ and the derivative with respect to $\\text{vech}(D_{k_2})$.\n",
        "\n",
        "$$\\text{cov}\\bigg(\\frac{\\delta l(\\theta | y)}{\\delta \\text{vech}(D_{k_2})},\\frac{\\delta l(\\theta | y)}{\\delta \\text{vech}(D_{k_2})}\\bigg)=\\frac{1}{2}\\mathcal{D}_{k_1}^+\\sum_{j=1}^{l_{k_2}}\\sum_{i=1}^{l_{k_1}}(R_{(k_1,k_2,i,j)}\\otimes R_{(k_1,k_2, i,j)}')\\mathcal{D}_{k_2}^{+'}$$\n",
        "\n",
        "\n",
        "\n",
        "Where $R_{(k_1,k_2,i,j)}=Z_{(k_1,i)}'(I+ZDZ')^{-1}Z_{(k_2,j)}=Z_{(k_1,i)}'Z_{(k_2,j)} - Z_{(k_1,i)}'ZD(I+Z'ZD)^{-1}Z_{(k_2,j)}$.\n",
        "\n",
        "----\n",
        "\n",
        "This function takes the following inputs:\n",
        "\n",
        "----\n",
        "\n",
        " - `k1`: The number of the first factor ($k_1$ in the above notation).\n",
        " - `k2`: The number of the second factor ($k_2$ in the above notation).\n",
        " - `nlevels`: A vector containing the number of levels for each factor, e.g. `nlevels=[3,4]` would mean the first factor has 3 levels and the second factor has 4 levels.\n",
        " - `nparams`: A vector containing the number of parameters for each factor, e.g. `nlevels=[2,1]` would mean the first factor has 2 parameters and the second factor has 1 parameter.\n",
        " - `ZtZ`: $Z$ transpose multiplied by $Z$.\n",
        " - `DinvIplusZtZD`: $D(I+Z'ZD)^{-1}$ in the above notation.\n",
        " - `invDupMatdict`: A dictionary of inverse duplication matrices such that `invDupMatdict[k]` = $\\mathcal{D}_k^+$\n",
        " \n",
        "----\n",
        "\n",
        "It returns as outputs:\n",
        "\n",
        "----\n",
        "\n",
        "- `covdldDk1dldk2`: The covariance between the derivative of the log likelihood with respect to $\\text{vech}(D_{k_1})$ and the derivative with respect to $\\text{vech}(D_{k_2})$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoAsQQtM4LLs",
        "colab_type": "code",
        "outputId": "2257f335-136b-4e95-afc2-d8ad9b613dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "import numba\n",
        "\n",
        "@numba.jit\n",
        "def get_covdldDk1Dk2(k1, k2, nlevels, nparams, ZtZ, DinvIplusZtZD, invDupMatdict):\n",
        "  \n",
        "  # Sum of R_(k1, k2, i, j) kron R_(k1, k2, i, j) over i and j \n",
        "  for i in np.arange(nlevels[k1]):\n",
        "\n",
        "    for j in np.arange(nlevels[k2]):\n",
        "      \n",
        "      # Get the indices for the k1th factor jth level\n",
        "      Ik1i = faclev_indices(k1, i, nlevels, nparams)\n",
        "      Ik2j = faclev_indices(k2, j, nlevels, nparams)\n",
        "\n",
        "      # Work out R_(k1, k2, i, j)\n",
        "      Rk1k2ij = ZtZ[np.ix_(Ik1i,Ik2j)] - (ZtZ[Ik1i,:] @ DinvIplusZtZD @ ZtZ[:,Ik2j])\n",
        "\n",
        "      # Work out Rk1k2ij kron Rk1k2ij\n",
        "      RkRt = np.kron(Rk1k2ij,Rk1k2ij.transpose())\n",
        "      \n",
        "      # Add together\n",
        "      if (i == 0) and (j == 0):\n",
        "      \n",
        "        RkRtSum = RkRt\n",
        "      \n",
        "      else:\n",
        "        \n",
        "        RkRtSum = RkRtSum + RkRt\n",
        "\n",
        "  # Multiply by duplication matrices and save\n",
        "  covdldDk1dldk2 = 1/2 * invDupMatdict[k1] @ RkRtSum @ invDupMatdict[k2].transpose()\n",
        "\n",
        "  # Return the result\n",
        "  return(covdldDk1dldk2)\n",
        "\n",
        "\n",
        "# Example\n",
        "\n",
        "invDupMatdict = dict()\n",
        "for i in np.arange(len(nparams)):\n",
        "  \n",
        "  invDupMatdict[i] = invDupMat(nparams[i])\n",
        "  \n",
        "t1 = time.time()\n",
        "examplecov = get_covdldDk1Dk2(0, 0, nlevels, nparams, ZtZ, DinvIplusZtZD, invDupMatdict)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "print(examplecov)\n",
        "k1 = 0\n",
        "k2 = 0\n",
        "\n",
        "t1 = time.time()\n",
        "examplecov = get_covdldDk1Dk2(k1, k2, nlevels, nparams, ZtZ, DinvIplusZtZD, invDupMatdict)\n",
        "t2 = time.time()\n",
        "\n",
        "print(t2-t1)\n",
        "print(examplecov)\n",
        "\n",
        "print(invDupMatdict[k1])\n",
        "\n",
        "# Check against alternative expression\n",
        "Ztmp = Z.toarray()\n",
        "\n",
        "IplusZDZt = np.eye(n) + Z @ D @ Z.transpose()\n",
        "\n",
        "invhalfIplusZDZt = scipy.linalg.sqrtm(np.linalg.inv(IplusZDZt))\n",
        "\n",
        "for j in np.arange(nlevels[k1]):\n",
        "  \n",
        "  Ikj = faclev_indices(k1, j, nlevels, nparams)\n",
        "\n",
        "  Tkj = Z[:,Ikj].transpose() @ invhalfIplusZDZt \n",
        "  \n",
        "  if j == 0:\n",
        "    \n",
        "    sumTkT = np.kron(Tkj,Tkj)\n",
        "  \n",
        "  else:\n",
        "    \n",
        "    sumTkT = np.kron(Tkj,Tkj) + sumTkT\n",
        "\n",
        "    \n",
        "for j in np.arange(nlevels[k2]):\n",
        "  \n",
        "  Ikj = faclev_indices(k2, j, nlevels, nparams)\n",
        "\n",
        "  Tkj = Z[:,Ikj].transpose() @ invhalfIplusZDZt \n",
        "  \n",
        "  Tkjt = Tkj.transpose()\n",
        "  \n",
        "  if j == 0:\n",
        "    \n",
        "    sumTtkTt = np.kron(Tkjt,Tkjt)\n",
        "  \n",
        "  else:\n",
        "    \n",
        "    sumTtkTt = np.kron(Tkjt,Tkjt) + sumTtkTt\n",
        "\n",
        "print(1/2 * invDupMatdict[k1] @ sumTkT @ sumTtkTt @ invDupMatdict[k2].transpose())\n",
        "print(np.abs(1/2 * invDupMatdict[k1] @ sumTkT @ sumTtkTt @ invDupMatdict[k2].transpose() - examplecov)/examplecov)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5755360126495361\n",
            "[[ 2.43466801 -0.75287721  0.23281371]\n",
            " [-0.75287721  0.79897441 -0.42214343]\n",
            " [ 0.23281371 -0.42214343  0.79183281]]\n",
            "0.025798559188842773\n",
            "[[ 2.43466801 -0.75287721  0.23281371]\n",
            " [-0.75287721  0.79897441 -0.42214343]\n",
            " [ 0.23281371 -0.42214343  0.79183281]]\n",
            "  (0, 0)\t1.0\n",
            "  (1, 1)\t0.5\n",
            "  (1, 2)\t0.5\n",
            "  (2, 3)\t1.0\n",
            "[[ 2.43466801-2.85390278e-16j -0.75287721+1.22468997e-16j\n",
            "   0.23281371-4.85934912e-17j]\n",
            " [-0.75287721+1.22468997e-16j  0.79897441+1.68434771e-15j\n",
            "  -0.42214343-1.04774292e-15j]\n",
            " [ 0.23281371-4.85934912e-17j -0.42214343-1.04774292e-15j\n",
            "   0.79183281+4.10853331e-15j]]\n",
            "[[ 6.10683139e-13 -5.81863543e-12  4.95177752e-10]\n",
            " [-5.82261696e-12  7.99701609e-11 -9.95284499e-12]\n",
            " [ 4.95177752e-10 -9.95047802e-12  4.28732227e-12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWnudhK-03e8",
        "colab_type": "code",
        "outputId": "655108a4-639e-4648-bf11-98c826489a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "# Useful scalars\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Number of factors, r\n",
        "r = len(nlevels)\n",
        "\n",
        "# Number of subjects, n\n",
        "n = Z.shape[0]\n",
        "\n",
        "# Number of random effects, q\n",
        "q = np.sum(np.dot(nparams,nlevels))\n",
        "\n",
        "# Number of fixed effects, p\n",
        "p = XtX.shape[0]\n",
        "\n",
        "# Initial estimates\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Inital beta\n",
        "beta = np.ones((p,1))\n",
        "\n",
        "# Work out e'e\n",
        "ete = ssr(YtX, YtY, XtX, beta)\n",
        "\n",
        "# Initial sigma2\n",
        "sigma2 = initSigma2(ete, n)\n",
        "\n",
        "# Inital D\n",
        "Zte = ZtY - (ZtX @ beta)\n",
        "\n",
        "# Dictionary version\n",
        "Ddict = dict()\n",
        "for k in np.arange(len(nparams)):\n",
        "  \n",
        "  Ddict[k] = makeDnnd(initDk(k, nlevels[k], ZtZ, Zte, sigma2))\n",
        "\n",
        "# Matrix version\n",
        "D = np.array([])\n",
        "for i in np.arange(len(nparams)):\n",
        "  \n",
        "  for j in np.arange(nlevels[i]):\n",
        "    \n",
        "    if i == 0 and j == 0:\n",
        "\n",
        "      D = Ddict[i]\n",
        "\n",
        "    else:\n",
        "\n",
        "      D = scipy.linalg.block_diag(D, Ddict[i])\n",
        "\n",
        "# Duplication matrices\n",
        "# ------------------------------------------------------------------------------\n",
        "invDupMatdict = dict()\n",
        "for i in np.arange(len(nparams)):\n",
        "  \n",
        "  invDupMatdict[i] = invDupMat(nparams[i])\n",
        "\n",
        "\n",
        "t1 = time.time()\n",
        "tol = np.zeros(1000)\n",
        "for z in np.arange(1000):\n",
        "  \n",
        "  # Matrices needed later by many calculations:\n",
        "  # ----------------------------------------------------------------------------\n",
        "  # X transpose e and Z transpose e\n",
        "  Xte = XtY - (XtX @ beta)\n",
        "  Zte = ZtY - (ZtX @ beta)\n",
        "  \n",
        "  # Inverse of (I+Z'ZD) multiplied by D\n",
        "  IplusZtZD = np.eye(q) + ZtZ @ D\n",
        "  DinvIplusZtZD = D @ np.linalg.inv(IplusZtZD)\n",
        "  \n",
        "  # Sum of squared residuals\n",
        "  ete = ssr(YtX, YtY, XtX, beta)\n",
        "  \n",
        "  # Derivatives\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  # Derivative wrt beta\n",
        "  dldB = get_dldB(sigma2, Xte, XtZ, DinvIplusZtZD, Zte)\n",
        "  \n",
        "  # Derivative wrt sigma^2\n",
        "  dldsigma2 = -n/(2*sigma2)  + 1/(2*(sigma2**2))*(ete - (Zte.transpose() @ DinvIplusZtZD @ Zte))\n",
        "  \n",
        "  # For each factor, factor k, work out dl/dD_k\n",
        "  dldDdict = dict()\n",
        "  for k in np.arange(len(nparams)):\n",
        "    # Store it in the dictionary\n",
        "    dldDdict[k] = get_dlDk(k, nlevels, nparams, ZtZ, Zte, sigma2, DinvIplusZtZD)\n",
        "\n",
        "    \n",
        "  # Covariances\n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  # Covariance of dl/dbeta\n",
        "  covdldB = (1/sigma2)*(XtX - (XtZ @ DinvIplusZtZD @ ZtX))\n",
        "\n",
        "  # Covariance of dl/dsigma2\n",
        "  covdldsigma2 = n/(2*(sigma2**2))\n",
        "  \n",
        "  covdldDdict = dict()\n",
        "  # For each pair of factors, factor k1 and k2, work out dl/dD_k\n",
        "  for k1 in np.arange(len(nparams)):\n",
        "\n",
        "    for k2 in np.arange(k1+1):\n",
        "\n",
        "      # Multiply by duplication matrices and save\n",
        "      covdldDdict[str(k1) + str(k2)] = get_covdldDk1Dk2(k1, k2, nlevels, nparams, ZtZ, DinvIplusZtZD, invDupMatdict)\n",
        "\n",
        "  covdldDdldsigmadict = dict()\n",
        "\n",
        "  # For each factor, factor k, work out cov(dl/dD_k, dl/dsigma)\n",
        "  for k in np.arange(len(nparams)):\n",
        "\n",
        "    # Sum of R_(k, j) over j\n",
        "    RkSum = np.zeros(nparams[k],nparams[k])\n",
        "\n",
        "    for j in np.arange(nlevels[k]):\n",
        "\n",
        "      # Get the indices for the kth factor jth level\n",
        "      Ikj = faclev_indices(k, j, nlevels, nparams)\n",
        "\n",
        "      # Work out R_(k, j)\n",
        "      Rkj = ZtZ[np.ix_(Ikj,Ikj)] - (ZtZ[Ikj,:] @ DinvIplusZtZD @ ZtZ[:,Ikj])\n",
        "\n",
        "      # Add together\n",
        "      RkSum = RkSum + Rkj\n",
        "\n",
        "    # Multiply by duplication matrices and save\n",
        "    covdldDdldsigmadict[k] = 1/(2*sigma2) * invDupMatdict[k] @ mat2vec(RkSum)\n",
        "\n",
        "  # \n",
        "  # ----------------------------------------------------------------------------\n",
        "\n",
        "  # Work out the total number of paramateres\n",
        "  tnp = np.int32(p + 1 + np.sum(nparams*(nparams+1)/2))\n",
        "\n",
        "  # Indices for submatrics corresponding to Dks\n",
        "  FishIndsDk = np.int32(np.cumsum(nparams*(nparams+1)/2) + p + 1)\n",
        "  FishIndsDk = np.insert(FishIndsDk,0,p+1)\n",
        "\n",
        "  # Construct the Fisher Information matrix\n",
        "  FisherInfoMat = np.zeros((tnp,tnp))\n",
        "\n",
        "  # Add dl/dbeta covariance\n",
        "  FisherInfoMat[np.ix_(np.arange(p),np.arange(p))] = covdldB\n",
        "\n",
        "  # Add dl/dsigma2 covariance\n",
        "  #print(covdldsigma2)\n",
        "  FisherInfoMat[p,p] = covdldsigma2\n",
        "\n",
        "  # Add dl/dsigma2 dl/dD covariance\n",
        "  for k in np.arange(len(nparams)):\n",
        "\n",
        "    # Work out number of covariance parameters estimated\n",
        "    qk = np.int32(nparams[k]*(nparams[k]+1)/2)\n",
        "\n",
        "    # Assign to the relevant block\n",
        "    FisherInfoMat[p, FishIndsDk[k]:FishIndsDk[k+1]] = covdldDdldsigmadict[k].reshape(qk)\n",
        "    FisherInfoMat[FishIndsDk[k]:FishIndsDk[k+1],p] = covdldDdldsigmadict[k].reshape(qk)\n",
        "\n",
        "  # Add dl/dD covariance\n",
        "  for k1 in np.arange(len(nparams)):\n",
        "\n",
        "    for k2 in np.arange(k1+1):\n",
        "\n",
        "      IndsDk1 = np.arange(FishIndsDk[k1],FishIndsDk[k1+1])\n",
        "      IndsDk2 = np.arange(FishIndsDk[k2],FishIndsDk[k2+1])\n",
        "      FisherInfoMat[np.ix_(IndsDk1, IndsDk2)] = covdldDdict[str(k1) + str(k2)]\n",
        "      FisherInfoMat[np.ix_(IndsDk2, IndsDk1)] = covdldDdict[str(k1) + str(k2)].transpose()\n",
        "\n",
        "  paramVector = np.concatenate((beta, np.array([[sigma2]])))\n",
        "  derivVector = np.concatenate((dldB, dldsigma2))\n",
        "\n",
        "  for k in np.arange(len(nparams)):\n",
        "\n",
        "    paramVector = np.concatenate((paramVector, mat2vech(Ddict[k])))\n",
        "    derivVector = np.concatenate((derivVector, mat2vech(dldDdict[k])))\n",
        "\n",
        "  paramVector = paramVector + (np.linalg.inv(FisherInfoMat) @ derivVector)\n",
        "  tol[z] = np.amax(np.abs(np.linalg.inv(FisherInfoMat) @ derivVector))\n",
        "\n",
        "  #print(paramVector)\n",
        "  beta = paramVector[0:p]\n",
        "  sigma2 = paramVector[p:(p+1)][0,0]\n",
        "\n",
        "  for k in np.arange(len(nparams)):\n",
        "\n",
        "    # Work out number of covariance parameters estimated\n",
        "    qk = np.int32(nparams[k]*(nparams[k]+1)/2)\n",
        "    \n",
        "    # Work out the new estimate of D_k\n",
        "    Ddict[k] = makeDnnd(vech2mat(paramVector[(p + 1 + k*qk):(p + 1 + (k+1)*qk)]))\n",
        "  \n",
        "  print(z)\n",
        "  z=z+1\n",
        "  for i in np.arange(len(nparams)):\n",
        "\n",
        "    for j in np.arange(nlevels[i]):\n",
        "      \n",
        "\n",
        "      if i == 0 and j == 0:\n",
        "\n",
        "        D = Ddict[i]\n",
        "\n",
        "      else:\n",
        "\n",
        "        D = scipy.linalg.block_diag(D, Ddict[i])\n",
        "        \n",
        "t2 = time.time()\n",
        "print(t2-t1)\n",
        "\n",
        "print(FisherInfoMat)\n",
        "#imshow(FisherInfoMat, \\\n",
        "#       interpolation='nearest', vmin=-5, vmax=5, aspect='auto')\n",
        "\n",
        "#plt.colorbar()\n",
        "\n",
        "plt.plot(np.arange(len(tol)),tol)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "48.62742304801941\n",
            "[[-3.37603174e-03  6.04755687e-04 -7.34969049e-04  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-8.14755438e-04 -2.10100608e+02  1.68764575e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.21688322e-03  1.68766980e+00 -1.95584485e+02  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.02735666e+01\n",
            "  -1.45134680e-04  3.10426301e-03  1.62672224e-02 -5.51273968e-04\n",
            "   1.68025418e-03 -2.15205182e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.45134680e-04\n",
            "   2.52735761e-04  1.43432904e-04 -5.59166834e-04  1.68746593e-04\n",
            "   3.98811132e-05 -6.70818519e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  3.10426301e-03\n",
            "   1.43432904e-04  1.34126095e-02  1.44130549e-02  3.98811132e-05\n",
            "   4.44539982e-04 -2.33177924e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.62672224e-02\n",
            "  -5.59166834e-04  1.44130549e-02  4.60357535e-01 -6.70818519e-05\n",
            "  -2.33177924e-04  1.12244950e-03]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.51273968e-04\n",
            "   1.68746593e-04  3.98811132e-05 -6.70818519e-05  1.09683604e-04\n",
            "  -3.15866798e-05  6.48030406e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.68025418e-03\n",
            "   3.98811132e-05  4.44539982e-04 -2.33177924e-04 -3.15866798e-05\n",
            "   1.89334613e-04 -1.34669527e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.15205182e-04\n",
            "  -6.70818519e-05 -2.33177924e-04  1.12244950e-03  6.48030406e-05\n",
            "  -1.34669527e-04  2.65223064e-04]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f57cbfe6f98>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpiSHkObQ7yN",
        "colab_type": "code",
        "outputId": "566a0aac-0961-4009-8a66-b4172d55fe0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(beta)\n",
        "\n",
        "print(RFXVar_REst)\n",
        "\n",
        "print(sigma2)\n",
        "print(Ddict[0])\n",
        "print(Ddict[1])\n",
        "\n",
        "\n",
        "print(\"U estimates (R)\")\n",
        "print(pd.read_csv('/Data/BLMM-testdata/estd_b.csv',header=None).values.reshape(23,2))\n",
        "print(\"U estimates (FS)\")\n",
        "print((DinvIplusZtZD @ Zte).reshape(23,2))\n",
        "print(\"U true\")\n",
        "print(pd.read_csv('/Data/BLMM-testdata/true_b.csv',header=None).values.reshape(23,2))\n",
        "\n",
        "plt.plot(np.arange(len(tol)),np.log10(tol))\n",
        "#print(tol)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.74261754]\n",
            " [ 1.98359777]\n",
            " [ 3.03135488]]\n",
            "[[ 1.24022707  0.36034323  0.          0.        ]\n",
            " [ 0.36034323  5.0719078   0.          0.        ]\n",
            " [ 0.          0.         20.68631165 -0.79595202]\n",
            " [ 0.          0.         -0.79595202  0.21015464]]\n",
            "0.9646341001965331\n",
            "[[ 3.19186856e+09 -3.09961133e+09]\n",
            " [-3.09961133e+09  3.01125992e+09]]\n",
            "[[ 8009725.82060706 -2008017.00320966]\n",
            " [-2008017.00320966  4001973.66815951]]\n",
            "U estimates (R)\n",
            "[[-0.16266309 -4.21724427]\n",
            " [ 1.41643004  1.41243783]\n",
            " [ 1.59527575 -1.58921871]\n",
            " [ 1.57314783  0.40497904]\n",
            " [ 1.29727291  0.93796984]\n",
            " [-1.20537013  2.19715236]\n",
            " [-0.62177368 -1.1845146 ]\n",
            " [-0.8982196  -1.34761407]\n",
            " [ 0.2105669   2.5381855 ]\n",
            " [-0.57561587 -3.43679857]\n",
            " [ 1.33808317  0.10724134]\n",
            " [-1.20676845 -2.43712662]\n",
            " [ 0.64262693  1.33629341]\n",
            " [-0.9916963  -0.08043681]\n",
            " [ 1.44782818  2.19618068]\n",
            " [-1.25815704  4.3074814 ]\n",
            " [ 0.27424557  0.87149959]\n",
            " [-0.78704685 -3.16062475]\n",
            " [-1.48916827 -0.24116661]\n",
            " [-0.47444214  3.13847411]\n",
            " [ 1.80633411  0.64368738]\n",
            " [ 2.06756827 -0.15742136]\n",
            " [-7.37272861  0.43752657]]\n",
            "U estimates (FS)\n",
            "[[-0.33513704 -4.21738743]\n",
            " [ 1.26607442  1.41248918]\n",
            " [ 1.44913787 -1.58906392]\n",
            " [ 1.42188129  0.40501152]\n",
            " [ 1.14305601  0.93802021]\n",
            " [-1.40151823  2.19722399]\n",
            " [-0.80549118 -1.18454668]\n",
            " [-1.08656821 -1.34749866]\n",
            " [ 0.03639427  2.5382593 ]\n",
            " [-0.7559801  -3.43673794]\n",
            " [ 1.18684241  0.10692303]\n",
            " [-1.39851186 -2.43711449]\n",
            " [ 0.47710061  1.33628891]\n",
            " [-1.18284383 -0.08039497]\n",
            " [ 1.29628792  2.19593543]\n",
            " [-1.45891958  4.30749818]\n",
            " [ 0.10324484  0.87149578]\n",
            " [-0.97120386 -3.16063352]\n",
            " [-1.68853173 -0.24118019]\n",
            " [-0.6607976   3.13842982]\n",
            " [ 0.04057416  0.6438366 ]\n",
            " [ 0.30172867 -0.15729378]\n",
            " [-9.14069229  0.43762989]]\n",
            "U true\n",
            "[[-0.19301143 -4.22666242]\n",
            " [ 1.43735476  1.40883489]\n",
            " [ 1.34711832 -1.58788242]\n",
            " [ 1.81825209  0.3910735 ]\n",
            " [ 1.25654173  0.94063103]\n",
            " [-1.22775617  2.19293973]\n",
            " [-0.79408731 -1.19281546]\n",
            " [-0.9898057  -1.35221872]\n",
            " [-0.03319243  2.53178969]\n",
            " [-0.65604937 -3.43139032]\n",
            " [ 1.42910277  0.11018046]\n",
            " [-0.96244844 -2.43071144]\n",
            " [ 0.70088294  1.33487455]\n",
            " [-1.3137399  -0.07784482]\n",
            " [ 1.45666816  2.19521647]\n",
            " [-1.3663195   4.30629966]\n",
            " [ 0.22757187  0.87208539]\n",
            " [-0.89571743 -3.14622337]\n",
            " [-1.51292639 -0.23242295]\n",
            " [-0.5588196   3.15054584]\n",
            " [ 1.85205881  0.64080503]\n",
            " [ 2.10133112 -0.1570577 ]\n",
            " [-7.27154738  0.44280999]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f57cbb1b9e8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhQP_ZYStIDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM_nXUHfJEd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.min(np.abs(np.linalg.inv(FisherInfoMat)[np.linalg.inv(FisherInfoMat)!=0])))\n",
        "print(paramVector)\n",
        "print(derivVector)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uyderceEM3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 0\n",
        "j = 3\n",
        "Z2 = Z.toarray()\n",
        "e = Y - X @ beta\n",
        "print(np.sum(np.sum(np.abs(Z2[:,faclev_indices(k, j, nlevels, nparams)].transpose() @ Z2 - ZtZ[faclev_indices(k, j, nlevels, nparams), :]))))\n",
        "\n",
        "print(np.sum(np.sum(np.abs(Z2[:,faclev_indices(k, j, nlevels, nparams)].transpose() @ e - Zte[faclev_indices(k, j, nlevels, nparams), :]))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOHZ6F2DU5e",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the Information matrix\n",
        "\n",
        "Fisher scoring requires the calculation of the information matrix, $\\mathcal{I}$. This is done using the method outlined in [Demidenko 2013](https://www.wiley.com/en-us/Mixed+Models%3A+Theory+and+Applications+with+R%2C+2nd+Edition-p-9781118091579) section 3.3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtC2_vLtsY2",
        "colab_type": "text"
      },
      "source": [
        "### Fisher Information Matrix function\n",
        "\n",
        "This function calculates the Fisher Information matrix.\n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Ty4TMRUjS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "Xtmp = np.random.randn(10,9)\n",
        "\n",
        "XkX = np.kron(Xtmp,Xtmp)\n",
        "\n",
        "#print(XkX.shape[0]^2)\n",
        "\n",
        "#print(XkX.shape[0]**2)\n",
        "#print(np.power(XkX.shape[0],2))\n",
        "\n",
        "R = (np.arange(XkX.shape[0]))\n",
        "#print(R)\n",
        "\n",
        "#print(R.shape)\n",
        "Rt = mat2vec(vec2mat(R).transpose())\n",
        "\n",
        "perm = np.argsort(Rt.reshape(Rt.shape[0]))\n",
        "\n",
        "K = np.eye(R.shape[0],R.shape[0])\n",
        "K=K[:,perm]\n",
        "\n",
        "#print(perm)\n",
        "\n",
        "imshow(K, \\\n",
        "   interpolation='nearest', aspect='auto',vmax=2,vmin=-1)\n",
        "plt.colorbar()\n",
        "\n",
        "#print(K @ R.reshape(R.shape[0],1) - Rt.reshape(R.shape[0],1))\n",
        "\n",
        "R = (np.arange(XkX.shape[1]))\n",
        "#print(R)\n",
        "\n",
        "#print(R.shape)\n",
        "Rt = mat2vec(vec2mat(R).transpose())\n",
        "\n",
        "perm = np.argsort(Rt.reshape(Rt.shape[0]))\n",
        "\n",
        "K2 = np.eye(R.shape[0],R.shape[0])\n",
        "K2=K2[:,perm]\n",
        "\n",
        "#print(perm)\n",
        "\n",
        "imshow(K2, \\\n",
        "   interpolation='nearest', aspect='auto',vmax=2,vmin=-1)\n",
        "plt.colorbar()\n",
        "\n",
        "#print(K2 @ R.reshape(R.shape[0],1) - Rt.reshape(R.shape[0],1))\n",
        "\n",
        "N = (K + np.eye(K.shape[0]))/2\n",
        "N2 = (K2 + np.eye(K2.shape[0]))/2\n",
        "\n",
        "#print(K.shape)\n",
        "#print(K2.shape)\n",
        "#print(XkX.shape)\n",
        "print((N @ XkX))\n",
        "#print((N @ XkX).shape)\n",
        "#print((XkX @ N2).shape)\n",
        "\n",
        "print((XkX @ N2)-(N @ XkX))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fRUGMZ96yKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtmp = np.random.randn(100,100)\n",
        "\n",
        "imshow(np.kron(Xtmp,Xtmp)[1:100,1:100], \\\n",
        "   interpolation='nearest', aspect='auto',vmax=2,vmin=-1)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr7WPsyU7LZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imshow(np.kron(Xtmp[1:10,1:10],Xtmp[1:10,1:10]), \\\n",
        "   interpolation='nearest', aspect='auto',vmax=2,vmin=-1)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nazpuh-A5vx",
        "colab_type": "text"
      },
      "source": [
        "## Scaling up the computation (Multiple voxels)\n",
        "\n",
        "This section has several implemented ideas for scaling up the computation to compute several similar models at once. For simplicity it is assumed here that X and Z are the same across voxels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYnjHWXBVmO",
        "colab_type": "text"
      },
      "source": [
        "### Toy data for Neighbouring voxel\n",
        "\n",
        "To assess the potential of the following ideas a toy data example is created below. The idea behind this is that we wish to calculate both the model in the example used in the previous sections and, additionally a similar model from a neighbouring voxel (variables related to the neighbouring voxel will have postfix `_n`). \n",
        "\n",
        "This is not a rigourous test, but just a toy example to see if we can lower the computational time in an example vaguely similar to what we may expect in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzUVeNWeDLmy",
        "colab_type": "text"
      },
      "source": [
        "#### Beta vector\n",
        "\n",
        "In a neighbouring voxel, we would expect similar Beta values but not necessarily the same. To simulate this, I have added some normal noise with variance, 1/2, to the original beta values to obtain a new beta value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hKzWuueBbeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given a beta vector this function makes a beta \n",
        "# vector for the neighbouring voxel\n",
        "def beta_n(beta):\n",
        "  return(beta + np.random.randn(beta.shape[0],1)/np.sqrt(2))\n",
        "\n",
        "# Example\n",
        "beta_True_n = beta_n(beta_True)\n",
        "  \n",
        "# print betas for comparison\n",
        "print(\"Beta for voxel 1\")\n",
        "print(beta_True)\n",
        "print(\"Beta for voxel 2\")\n",
        "print(beta_True_n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQFQbRLDKmq",
        "colab_type": "text"
      },
      "source": [
        "#### b vector\n",
        "\n",
        "In a neighbouring voxel, we may also expect similar b values. To simulate this, I have added some normal noise with variance, 1/2, to the original b values to obtain a new b value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsyjopBUFNt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given a b vector this function makes a b\n",
        "# vector for the neighbouring voxel\n",
        "def b_n(b):\n",
        "  return(b + np.random.randn(b.shape[0],1)/np.sqrt(2))\n",
        "\n",
        "# Example\n",
        "b_True_n = b_n(b_True)\n",
        "  \n",
        "# print bs for comparison\n",
        "print(\"b for voxel 1 (first 5 elements)\")\n",
        "print(b_True[1:5])\n",
        "print(\"b for voxel 2 (first 5 elements)\")\n",
        "print(b_True_n[1:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFb5eO8jBWSf",
        "colab_type": "text"
      },
      "source": [
        " #### Y vector (New response)\n",
        " \n",
        "I now generate a new response vector with the new beta and b values for the neighbouring voxel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V59rDqEGU5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neighbouring voxels response vector\n",
        "Y_n = np.matmul(X,beta_True_n)+Z*b_True_n+np.random.randn(1000,1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fjCdxgH3aV",
        "colab_type": "text"
      },
      "source": [
        "### Product Matrices\n",
        "\n",
        "All products of matrices are calculated beforehand as it is both more computationally efficient and also similar to the setting we are interested in. For the neighbouring voxel only those involving the Y vector (response) need be recalculated as X and Z have not changed between voxel in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnx7aj6OH3tA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z tranpose Y_n\n",
        "ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "# X tranpose Y_n\n",
        "XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "# Y_n tranpose X\n",
        "YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "# Y_n transpose Z\n",
        "YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "# Y_n tranpose Y_n\n",
        "YtY_n=cvxopt.matrix.trans(Y_n)*Y_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqbOzGU-It3f",
        "colab_type": "text"
      },
      "source": [
        "#### Time efficiency\n",
        "\n",
        "What is really important here is not the estimate values (assuming they are correct), but the time taken to do this for both voxels. In the below, the two voxels are estimated 100 times twice, once reusing the estmate from the first voxel in estimating the second, and once computing the voxels completely seperately.\n",
        "\n",
        "**Conclusion:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OM3CTjNJXp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KowcFHBSSAIa",
        "colab_type": "text"
      },
      "source": [
        "#### Idea 1: Broadcast everything\n",
        "\n",
        "The title for this idea speaks for itself. The `scipy` library does not allow for broadcasting of sparse matrices, however the libraries `sparse` and `dask` supposedly do (little documentation is available for this however). If these can be used for broadcasting then this algorithm might be reasonably streamlinable for large numbers of voxels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkCJOA2DkdNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}